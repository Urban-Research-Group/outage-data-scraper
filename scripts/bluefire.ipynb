{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2017.csv', encoding='utf-16')\n",
    "df_18 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2018.csv', encoding='utf-16')\n",
    "df_19 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2019.csv', encoding='utf-16')\n",
    "df_20 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2020.csv', encoding='utf-16')\n",
    "df_21 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2021.csv', encoding='utf-16')\n",
    "df_22 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2022.csv', encoding='utf-16')\n",
    "df_23 = pd.read_csv('/Users/xuanedx1/github/outage-data-scraper/data/bluefire/POUS_Export_CityByUtility_Raw_2023.csv', encoding='utf-16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UtilityName</th>\n",
       "      <th>StateName</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>CityName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>CustomersTracked</th>\n",
       "      <th>CustomersOut</th>\n",
       "      <th>RecordDateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 Rivers Electric Coop</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>20003.0</td>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-17 16:48:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 Rivers Electric Coop</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>20003.0</td>\n",
       "      <td>983</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-29 05:28:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 Rivers Electric Coop</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>20003.0</td>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-24 05:48:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 Rivers Electric Coop</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>20003.0</td>\n",
       "      <td>983</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-06 05:05:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 Rivers Electric Coop</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>20003.0</td>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-22 05:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077308</th>\n",
       "      <td>Y-W Electric Association</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Dundy</td>\n",
       "      <td>Sanborn</td>\n",
       "      <td>31057.0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-25 21:25:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077309</th>\n",
       "      <td>Y-W Electric Association</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Dundy</td>\n",
       "      <td>Sanborn</td>\n",
       "      <td>31057.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-10-30 21:39:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077310</th>\n",
       "      <td>Y-W Electric Association</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Dundy</td>\n",
       "      <td>Sanborn</td>\n",
       "      <td>31057.0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-30 22:01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077311</th>\n",
       "      <td>Y-W Electric Association</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Dundy</td>\n",
       "      <td>Sanborn</td>\n",
       "      <td>31057.0</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-06 17:12:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077312</th>\n",
       "      <td>Y-W Electric Association</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Dundy</td>\n",
       "      <td>Sanborn</td>\n",
       "      <td>31057.0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-06 17:59:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12077313 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       UtilityName StateName CountyName CityName  CountyFIPS  \\\n",
       "0           4 Rivers Electric Coop    Kansas   Anderson  Unknown     20003.0   \n",
       "1           4 Rivers Electric Coop    Kansas   Anderson  Unknown     20003.0   \n",
       "2           4 Rivers Electric Coop    Kansas   Anderson  Unknown     20003.0   \n",
       "3           4 Rivers Electric Coop    Kansas   Anderson  Unknown     20003.0   \n",
       "4           4 Rivers Electric Coop    Kansas   Anderson  Unknown     20003.0   \n",
       "...                            ...       ...        ...      ...         ...   \n",
       "12077308  Y-W Electric Association  Nebraska      Dundy  Sanborn     31057.0   \n",
       "12077309  Y-W Electric Association  Nebraska      Dundy  Sanborn     31057.0   \n",
       "12077310  Y-W Electric Association  Nebraska      Dundy  Sanborn     31057.0   \n",
       "12077311  Y-W Electric Association  Nebraska      Dundy  Sanborn     31057.0   \n",
       "12077312  Y-W Electric Association  Nebraska      Dundy  Sanborn     31057.0   \n",
       "\n",
       "          CustomersTracked  CustomersOut       RecordDateTime  \n",
       "0                      982             0  2023-03-17 16:48:34  \n",
       "1                      983             0  2023-03-29 05:28:08  \n",
       "2                      982             0  2023-05-24 05:48:41  \n",
       "3                      983             0  2023-06-06 05:05:54  \n",
       "4                      981             0  2023-06-22 05:55:10  \n",
       "...                    ...           ...                  ...  \n",
       "12077308                67             0  2023-10-25 21:25:03  \n",
       "12077309                67             1  2023-10-30 21:39:07  \n",
       "12077310                67             0  2023-10-30 22:01:36  \n",
       "12077311                67             2  2023-11-06 17:12:50  \n",
       "12077312                67             0  2023-11-06 17:59:46  \n",
       "\n",
       "[12077313 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    # Ensure RecordDateTime is a datetime type\n",
    "    df['RecordDateTime'] = pd.to_datetime(df['RecordDateTime'])\n",
    "    \n",
    "    # Extract and create year, month, day, hour, and date columns\n",
    "    df['Year'] = df['RecordDateTime'].dt.year\n",
    "    df['Month'] = df['RecordDateTime'].dt.month\n",
    "    df['Day'] = df['RecordDateTime'].dt.day\n",
    "    df['Hour'] = df['RecordDateTime'].dt.hour\n",
    "    df['Date'] = df['RecordDateTime'].dt.date  # Date without time component\n",
    "    \n",
    "    return df\n",
    "\n",
    "def aggregate(df):\n",
    "    aggregation_rules = {\n",
    "        'CustomersTracked': 'max',\n",
    "        'CustomersOut': 'max',\n",
    "    }\n",
    "    \n",
    "    # Group by the specified columns plus the datetime components and aggregate\n",
    "    aggregated_df = df.groupby(['UtilityName', 'StateName', 'CountyName', 'CountyFIPS', 'Date', 'Year', 'Month', 'Day', 'Hour'], as_index=False).agg(aggregation_rules)\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=[2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "for year,df in [[2017, df_17], [2018, df_18], [2019, df_19], [2020, df_20], [2021, df_21], [2022, df_22], [2023, df_23]]:\n",
    "    df_t = transform(df)\n",
    "    df_agg = aggregate(df_t)\n",
    "    df_agg.to_csv(f'/Users/xuanedx1/github/outage-data-scraper/data/bluefire/by_hour_{year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = df_23.query('StateName == \"Georgia\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/7cy055ys1yz5dcj9cwbxbw8h0000gp/T/ipykernel_1105/1150667029.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ga['RecordDateTime'] = pd.to_datetime(ga['RecordDateTime'].copy())\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type date is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/site-packages/altair/vegalite/v5/api.py:2975\u001b[0m, in \u001b[0;36mChart.to_dict\u001b[0;34m(self, validate, format, ignore, context)\u001b[0m\n\u001b[1;32m   2971\u001b[0m     copy\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mInlineData(values\u001b[38;5;241m=\u001b[39m[{}])  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Chart, copy)\u001b[38;5;241m.\u001b[39mto_dict(\n\u001b[1;32m   2973\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, ignore\u001b[38;5;241m=\u001b[39mignore, context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m   2974\u001b[0m     )\n\u001b[0;32m-> 2975\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/site-packages/altair/vegalite/v5/api.py:950\u001b[0m, in \u001b[0;36mTopLevelMixin.to_dict\u001b[0;34m(self, validate, format, ignore, context)\u001b[0m\n\u001b[1;32m    948\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    949\u001b[0m original_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(copy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, Undefined)\n\u001b[0;32m--> 950\u001b[0m copy\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Undefined:\n\u001b[1;32m    953\u001b[0m     context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m original_data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/site-packages/altair/vegalite/v5/api.py:122\u001b[0m, in \u001b[0;36m_prepare_data\u001b[0;34m(data, context)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# consolidate inline data to top-level datasets\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data_transformers\u001b[38;5;241m.\u001b[39mconsolidate_datasets:\n\u001b[0;32m--> 122\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# if data is still not a recognized type, then return\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mdict\u001b[39m, core\u001b[38;5;241m.\u001b[39mData)):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/site-packages/altair/vegalite/v5/api.py:86\u001b[0m, in \u001b[0;36m_consolidate_data\u001b[0;34m(data, context)\u001b[0m\n\u001b[1;32m     83\u001b[0m         kwds \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Undefined:\n\u001b[0;32m---> 86\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43m_dataset_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     data \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mNamedData(name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     88\u001b[0m     context\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})[name] \u001b[38;5;241m=\u001b[39m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/site-packages/altair/vegalite/v5/api.py:59\u001b[0m, in \u001b[0;36m_dataset_name\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;241m==\u001b[39m [{}]:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m values_json \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m hsh \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39mmd5(values_json\u001b[38;5;241m.\u001b[39mencode())\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m hsh\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/outage-data-scraper/lib/python3.8/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type date is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# Disable the max rows limit\n",
    "alt.data_transformers.disable_max_rows()\n",
    "# Assuming df is your DataFrame and already loaded\n",
    "\n",
    "# Convert RecordDateTime to datetime if not already\n",
    "ga['RecordDateTime'] = pd.to_datetime(ga['RecordDateTime'].copy())\n",
    "\n",
    "# Now create the chart\n",
    "chart = alt.Chart(ga).mark_line().encode(\n",
    "    x='RecordDateTime:T',  # T indicates temporal (time) data\n",
    "    y='CustomersOut:Q',    # Q indicates quantitative data\n",
    "    color='CountyName:N'   # N indicates nominal (categorical) data\n",
    "    # tooltip=['UtilityName:N', 'StateName:N', 'CountyName:N', 'CustomersOut:Q', 'RecordDateTime:T']\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    title='Customers Out Over Time by State'\n",
    ").interactive()\n",
    "\n",
    "chart.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
